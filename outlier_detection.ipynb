{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data from duplicates and missing values, we can now start to detect outliers.  \n",
    "First we will import modules and load the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataframes/final_cleaned.csv').iloc[:,1:]\n",
    "df.shape\n",
    "\n",
    "cat_cols = ['company_name','company_about', 'founded', 'business model','employees','product stage','status','funding stage','succeeded']\n",
    "num_cols = ['total_raised','total_rounds', 'investors','ipo_price', 'geo_market_per']\n",
    "tag_cols = [col for col in df.columns if col.startswith('tag_')]\n",
    "targetmarket_cols = [col for col in df.columns if col.startswith('targetmarket_')]\n",
    "sector_list = [col for col in df.columns if col.startswith(\"sector_\")]\n",
    "target_ind_list = [col for col in df.columns if col.startswith(\"industry_\")]\n",
    "technology_list = [col for col in df.columns if col.startswith(\"technology_\")]\n",
    "bin_cols = tag_cols + targetmarket_cols + sector_list + target_ind_list + technology_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deduced the relationship between columns can be seen as a graph, where each column presented as a **node**.  \n",
    "To define the edges, we will use the **pandas.DataFrame.corr()** function,  \n",
    "Each of the **edges** is a pair of highly correlated columns - above 0.7 correlation value:  \n",
    "$$ \\mid\\text{pearson correlation [i][j]}\\mid > 0.7 \\quad\\implies\\quad (\\text{columns[i]},\\text{columns[j]}) \\in E$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used NetworkX module to make the graph and run the algorithms on it.  \n",
    "[NetworkX module docs](https://networkx.org/documentation/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install networkx seaborn \n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use <b>SCC algorithm</b> to find all strongly connected component in the graph, which represents <b>highly correlated columns</b>.  \n",
    "For the most parts, our columns are structured in a way that theres a <b>certain inclusive tag</b> which can represent the exclusive tags that are highly correlated with.    \n",
    "Between highly correlated chronic disease tag and celiac or diabetes tag one is more inclusive than the other. Therefore we will remove the exclusive ones.  \n",
    "In our graph representation:  \n",
    "$$\n",
    "\\text{an inclusive column} \\iff {deg(\\text{inclusive column}) > deg(\\text{exclusive column})} \n",
    "$$ \n",
    "For each scc, we can infer and determine that the vertex with the highest degree can represent the other exclusive tags.  \n",
    "\n",
    "If we will run SCC algorithm, we will get the number of highly correlated 'representitives' that will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e is a list of highly correlated pairs of tags\n",
    "# We will use pandas corr function to find all correlations which absolute value greater than 0.7\n",
    "\n",
    "def get_edges(df, v):\n",
    "    df_cor = df[v].corr()\n",
    "    E = []\n",
    "\n",
    "    for i in range(len(df_cor.columns)):\n",
    "        for j in range(i+1, len(df_cor.columns)):\n",
    "            if abs(df_cor.iloc[i,j]) > 0.7:\n",
    "                E.append((df_cor.columns[i], df_cor.columns[j]))\n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Runtime ~4.5 minutes\n",
    "\n",
    "def scc(v, e):\n",
    "    \"\"\"\n",
    "    Calculates the number of connected components in a graph with vertices v and edges e.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a graph with v and e\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(v)\n",
    "    G.add_edges_from(e)\n",
    "    # Calculate the number of connected components\n",
    "    return nx.number_connected_components(G)\n",
    "\n",
    "scc(bin_cols, get_edges(df, bin_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a graph\n",
    "\n",
    "def make_graph(V, E):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(V)\n",
    "    G.add_edges_from(E)\n",
    "    return G\n",
    "\n",
    "# function to return the max degree of a graph\n",
    "\n",
    "def get_max_degree(G):\n",
    "    max_degree = 0\n",
    "    v = ''\n",
    "    for vertex in G.nodes():\n",
    "        if G.degree(vertex) > max_degree:\n",
    "            max_degree = G.degree(vertex)\n",
    "            v = vertex\n",
    "    # print(max_degree)\n",
    "    return v, max_degree\n",
    "    \n",
    "# function to remove all exclusive vertices\n",
    "# Each iteration we find the vertex with max degree and remove all its adjacent vertices\n",
    "\n",
    "def get_new_g(v,e):\n",
    "    G = make_graph(v,e)\n",
    "\n",
    "    v_name, max_degree = get_max_degree(G)\n",
    "    removed = 0\n",
    "\n",
    "    G2 = G.copy()\n",
    "    while max_degree > 0:\n",
    "\n",
    "        for adj in G.adj[v_name]:\n",
    "            G2.remove_node(adj)\n",
    "            removed += 1\n",
    "            \n",
    "        G = G2.copy()\n",
    "        v_name, max_degree = get_max_degree(G)\n",
    "\n",
    "    print(f\"Total vertices removed: {removed}\\n\\n\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the graph of the binary columns and run our algorithm on it.  \n",
    "After the algorithm is done, the returned graph will include only **not** correlated columns.  \n",
    "We will use the returned nodes to reduce the correlated columns in the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime ~4.5 minutes\n",
    "\n",
    "new_G = get_new_g(bin_cols,get_edges(df, bin_cols))\n",
    "\n",
    "new_cols = list(new_G.nodes())\n",
    "\n",
    "reduced_cols_df = df[cat_cols + num_cols + new_cols]\n",
    "print(f'New dataframe shape: {reduced_cols_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the correlated columns that doesn't contribute to the prediction, we can now look on the remaining columns:  \n",
    "</br>\n",
    "In our mostly binary dataset case, the outlier columns can be detected by the <b>sum of the 1 values in the column</b> - the column sum.  \n",
    "After getting the sum of the column, we will use the following function on the values:  \n",
    "$$\n",
    "\\text{value} = \\frac{\\text{number of 'succesful' companies with the tag}}{\\text{total companies with the tag}}\n",
    "$$\n",
    "We will then create a secondary dataset with the columns and their value.  \n",
    "And check both <b>Z score</b> and <b>IQR</b> methods on the secondary dataset to detect the outliers columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by calculating the columns sum and run the above function on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_dict = {}\n",
    "for col in new_cols:\n",
    "    sum_dict.update({col: reduced_cols_df[col].sum()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Runtime ~10 minutes\n",
    "# %pip install tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "temp_df = reduced_cols_df[reduced_cols_df['succeeded'] == 1]\n",
    "\n",
    "new_values = {}\n",
    "\n",
    "with tqdm.tqdm(total=len(new_cols)) as pbar:\n",
    "    for col in new_cols:\n",
    "        counter = 0\n",
    "        \n",
    "        for row in temp_df[col]:\n",
    "            if row == 1:\n",
    "                counter += 1\n",
    "        pbar.update(1)\n",
    "        new_values[col] = counter / sum_dict[col]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.hist(new_values.values(), bins=100)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see high rates of 1 and 0 in the new values.  \n",
    "This means that those columns are not helpful for our analysis, either the sum was 0 or only a small amount of companies had the tag and succeeded - we will drop them.  \n",
    "In addition, we can see a Gaussian bell distribution between values ~ 0.5 < X < 1  \n",
    "\n",
    "Lets try to explore our data first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.DataFrame(new_values.values(), index=new_values.keys(), columns = ['value'])\n",
    "norm_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mean of the normalized values is ~0.71, and the standard deviation is ~0.23 - as we observed from the Gaussian distribution.  \n",
    "We will now remove the columns with values 1 and 0 manualy to avoid biased mean and std values for a better outlier detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_normalized = {}\n",
    "for col, val in new_values.items():\n",
    "    if val != 0 and val != 1 :\n",
    "        new_normalized[col] = val\n",
    "        \n",
    "print(f'Total columns before: {len(new_values)}')\n",
    "print(f'Total columns after: {len(new_normalized)}\\n')\n",
    "\n",
    "new_norm_ser = pd.DataFrame(new_normalized.values(), index=new_normalized.keys(), columns = ['value'])\n",
    "new_norm_ser.describe()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing 1 and 0 values, we remain with 1980 columns.  \n",
    "Lets normalize the data and plot to see the bell better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = new_norm_ser['value'].sum()\n",
    "new_norm_ser['normalized_value'] = new_norm_ser['value'] / total\n",
    "new_norm_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_norm_ser['normalized_value'], bins=30)\n",
    "plt.xlabel('Normalized value')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start detecting the outliers with the z_score and IQR methods.  \n",
    "We will start by running a z_score method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection_zscore(df_orig) -> pd.Series:\n",
    "    \n",
    "    df = df_orig.copy()\n",
    "    z_score = (df['normalized_value'] - df['normalized_value'].mean()) / df['normalized_value'].std()\n",
    "    outliers = abs(z_score) > 3\n",
    "\n",
    "    return outliers\n",
    "\n",
    "z_score_outliers = outlier_detection_zscore(new_norm_ser)\n",
    "z_score_df = new_norm_ser[z_score_outliers == False]\n",
    "\n",
    "print(z_score_outliers.value_counts())\n",
    "print(f\"Z Score detected {z_score_outliers.shape[0] - z_score_df.shape[0]} outliers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(z_score_df['normalized_value'], bins=30)\n",
    "plt.xlabel('Normalized value')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_cols_reduction = reduced_cols_df.copy()\n",
    "print(f'Before: {df_after_cols_reduction.shape}')\n",
    "\n",
    "not_outliers_bin_cols = list(z_score_df.index)\n",
    "df_after_cols_reduction = df_after_cols_reduction[cat_cols + num_cols + not_outliers_bin_cols]\n",
    "print(f'After: {df_after_cols_reduction.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after <b>Z Score</b> detection method, we removed 17 columns.  \n",
    "\n",
    "Now, we will try to use IQR method, and see the differences:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_norm = new_norm_ser.copy()\n",
    "    \n",
    "Q1 = np.percentile(iqr_norm['normalized_value'], 25)\n",
    "Q3 = np.percentile(iqr_norm['normalized_value'], 75)\n",
    "IQR = 1.5*(Q3 - Q1)\n",
    "\n",
    "lower_bound = Q1 - IQR\n",
    "upper_bound = Q3 + IQR\n",
    "\n",
    "iqr_norm.loc[(iqr_norm[\"normalized_value\"]> upper_bound) | (iqr_norm[\"normalized_value\"]<lower_bound), 'iqr_outliers'] = True\n",
    "iqr_norm.loc[(iqr_norm[\"normalized_value\"]< upper_bound) & (iqr_norm[\"normalized_value\"]>lower_bound), 'iqr_outliers'] = False\n",
    "\n",
    "print(iqr_norm['iqr_outliers'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_df = iqr_norm.copy()\n",
    "for row in iqr_df.iterrows():\n",
    "    if row[1]['iqr_outliers'] == True:\n",
    "        iqr_df.drop(row[0], axis=0, inplace=True)\n",
    "\n",
    "plt.hist(iqr_df['normalized_value'], bins=30)\n",
    "plt.xlabel('Normalized value')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we will plot both methods's results with sns.kdeplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_cols_reduction = reduced_cols_df.copy()\n",
    "print(f'Before: {df_after_cols_reduction.shape}')\n",
    "not_outliers_bin_cols = list(iqr_df.index)\n",
    "df_after_cols_reduction = df_after_cols_reduction[cat_cols + num_cols + not_outliers_bin_cols]\n",
    "print(f'After: {df_after_cols_reduction.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after <b>IQR</b> detection method, we removed 67 columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.kdeplot(z_score_df['normalized_value'], shade=True)\n",
    "sns.kdeplot(iqr_df['normalized_value'], shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "After checking both methods for detecting outliers - Z score and IQR, \n",
    "We observed that Z score results were inferior to IQR results.  \n",
    "Therefore, we chose to use IQR detection method to remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_cols_reduction.to_csv('data/dataframes/df_after_cols_reduction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we took the cleaned dataframe and tried to reduce the dimentionality of it.  \n",
    "We did it firstly by removing highly correlated columns that gave no benefit to the prediction,  \n",
    "Then we used the columns sum to find statistically significant insights and removed biased column,  \n",
    "And lastly we used the z_score and IQR methods to detect outliers - we went with IQR method.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we will run PCA algorithm on the remaining columns to reduce dimentions even better: [PCA](pca.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d8353be1dd092e57b3f2779bafc40fd4a1c87861698b48ff47a5f1df7325f59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
