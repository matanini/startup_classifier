{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary dataframe shape: (10070, 1856)\n",
      "PCA dataframe shape: (10070, 26)\n"
     ]
    }
   ],
   "source": [
    "bin_df = pd.read_csv('data/dataframes/df_after_cols_reduction3.csv').iloc[:,1:]\n",
    "pca_df = pd.read_csv('data/dataframes/pca_df.csv').iloc[:,1:]\n",
    "\n",
    "print(f'Binary dataframe shape: {bin_df.shape}')\n",
    "print(f'PCA dataframe shape: {pca_df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1620.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_rate = 4000000\n",
    "\n",
    "# bin_df.loc[(bin_df[\"status\"] == 1) & (bin_df['total_raised'] >= success_rate), 'suceeded'] = 1\n",
    "# bin_df.loc[(bin_df[\"status\"] == 0) | (bin_df['total_raised'] < success_rate), 'suceeded'] = 0\n",
    "\n",
    "bin_df.suceeded.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['company_name', 'company_about','founded', 'business model','employees','product stage','status','fund_stage','suceeded']\n",
    "num_cols = ['total_raised','total_rounds', 'investors','ipo_price', 'geo_market_per']\n",
    "tag_cols = [col for col in bin_df.columns if col.startswith('tag_')]\n",
    "targetmarket_cols = [col for col in bin_df.columns if col.startswith('targetmarket_')]\n",
    "sector_list = [col for col in bin_df.columns if col.startswith(\"sector_\")]\n",
    "target_ind_list = [col  for col in bin_df.columns if col.startswith(\"industry_\")]\n",
    "technology_list = [col  for col in bin_df.columns if col.startswith(\"technology_\")]\n",
    "\n",
    "\n",
    "pca_cols = [col for col in pca_df.columns if col not in cat_cols and col not in num_cols]\n",
    "bin_cols = tag_cols + targetmarket_cols + sector_list + target_ind_list + technology_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cols : 9\n",
      "Numerical cols : 5\n",
      "Tag cols : 1571\n",
      "Targetmarket cols : 116\n",
      "Sector cols : 41\n",
      "Industry cols : 81\n",
      "Technology cols : 79\n",
      "Total binary cols : 1888\n",
      "Toatl PCA cols : 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Categorical cols : {len(cat_cols)}\")\n",
    "print(f\"Numerical cols : {len(num_cols)}\")\n",
    "print(f\"Tag cols : {len(tag_cols)}\")\n",
    "print(f\"Targetmarket cols : {len(targetmarket_cols)}\")\n",
    "print(f\"Sector cols : {len(sector_list)}\")\n",
    "print(f\"Industry cols : {len(target_ind_list)}\")\n",
    "print(f\"Technology cols : {len(technology_list)}\")\n",
    "print(f\"Total binary cols : {len(bin_cols)}\")\n",
    "print(f\"Toatl PCA cols : {len(pca_cols)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape: (8056, 1786)\n",
      "ytrain shape: (8056,)\n",
      "Xtrain_pca shape: (8056, 17)\n",
      "ytrain_pca shape: (8056,)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(bin_df[num_cols + bin_cols], bin_df['suceeded'],test_size = 0.2, stratify=bin_df['suceeded'])\n",
    "Xtrain_pca, Xtest_pca, ytrain_pca, ytest_pca = train_test_split(pca_df[num_cols + pca_cols], bin_df['suceeded'],test_size = 0.2, stratify=bin_df['suceeded'])\n",
    "\n",
    "print(f\"Xtrain shape: {Xtrain.shape}\")\n",
    "print(f\"ytrain shape: {ytrain.shape}\")\n",
    "\n",
    "print(f\"Xtrain_pca shape: {Xtrain_pca.shape}\")\n",
    "print(f\"ytrain_pca shape: {ytrain_pca.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=150)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter = 150)\n",
    "lr.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_pred = lr.predict(Xtrain)\n",
    "ytest_pred = lr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results:\n",
      "accuracy is: 0.7848808341608738\n",
      "precision is: 0.4001384562132226\n",
      "recall is: 1.0\n",
      "f1 is: 0.5715698393077874\n",
      "---------------------\n",
      "Test results:\n",
      "accuracy is: 0.7994041708043694\n",
      "precision is: 0.417027417027417\n",
      "recall is: 1.0\n",
      "f1 is: 0.5885947046843177\n"
     ]
    }
   ],
   "source": [
    "print(\"Train results:\")\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"precision is:\",metrics.precision_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"recall is:\",metrics.recall_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"f1 is:\",metrics.f1_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"---------------------\")\n",
    "print(\"Test results:\")\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"precision is:\",metrics.precision_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"recall is:\",metrics.recall_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"f1 is:\",metrics.f1_score(y_pred = ytest_pred, y_true = ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train results:  \n",
    "accuracy is: 0.7919563058589871  \n",
    "precision is: 0.4092351075079309  \n",
    "recall is: 1.0  \n",
    "f1 is: 0.5807903951975988  \n",
    "\n",
    "Test results:  \n",
    "accuracy is: 0.79493545183714  \n",
    "precision is: 0.41251778093883357  \n",
    "recall is: 1.0  \n",
    "f1 is: 0.5840886203423967  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240, 1856)\n",
      "1620.0\n"
     ]
    }
   ],
   "source": [
    "# create a new dataframe with suceeded = 1 and suceeded = 0\n",
    "\n",
    "bin_df_suceeded = bin_df[bin_df['suceeded'] == 1]\n",
    "bin_df_failed = bin_df[bin_df['suceeded'] == 0]\n",
    "\n",
    "size = bin_df_suceeded.shape[0]\n",
    "bin_df_fialed_sampled = bin_df_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "equal_df = pd.concat([bin_df_suceeded, bin_df_fialed_sampled])\n",
    "\n",
    "print(equal_df.shape)\n",
    "print(equal_df['suceeded'].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(equal_df[num_cols + bin_cols], equal_df['suceeded'],test_size = 0.2)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(Xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_pred = lr.predict(Xtrain)\n",
    "ytest_pred = lr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results:\n",
      "accuracy is: 0.8814878892733564\n",
      "precision is: 0.8098542678695351\n",
      "recall is: 1.0\n",
      "f1 is: 0.8949386503067485\n",
      "---------------------\n",
      "Test results:\n",
      "accuracy is: 0.8737024221453287\n",
      "precision is: 0.792022792022792\n",
      "recall is: 1.0\n",
      "f1 is: 0.8839427662957074\n"
     ]
    }
   ],
   "source": [
    "print(\"Train results:\")\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"precision is:\",metrics.precision_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"recall is:\",metrics.recall_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"f1 is:\",metrics.f1_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"---------------------\")\n",
    "print(\"Test results:\")\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"precision is:\",metrics.precision_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"recall is:\",metrics.recall_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"f1 is:\",metrics.f1_score(y_pred = ytest_pred, y_true = ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape: (2592, 1842)\n",
      "ytrain shape: (2592,)\n",
      "Xtest shape: (648, 1842)\n",
      "ytest shape: (648,)\n",
      "\n",
      "\n",
      "Train results:\n",
      "accuracy is: 0.8969907407407407\n",
      "precision is: 0.9033515198752923\n",
      "recall is: 0.8901689708141322\n",
      "f1 is: 0.8967117988394585\n",
      "---------------------\n",
      "Test results:\n",
      "accuracy is: 0.6882716049382716\n",
      "precision is: 0.6847133757961783\n",
      "recall is: 0.6761006289308176\n",
      "f1 is: 0.680379746835443\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(equal_df[bin_cols], equal_df['suceeded'],test_size = 0.2)\n",
    "\n",
    "print(f\"Xtrain shape: {Xtrain.shape}\")\n",
    "print(f\"ytrain shape: {ytrain.shape}\")\n",
    "print(f\"Xtest shape: {Xtest.shape}\")\n",
    "print(f\"ytest shape: {ytest.shape}\")\n",
    "\n",
    "lr = LogisticRegression(max_iter=1500)\n",
    "lr.fit(Xtrain, ytrain)\n",
    "\n",
    "ytrain_pred = lr.predict(Xtrain)\n",
    "ytest_pred = lr.predict(Xtest)\n",
    "\n",
    "print(\"\\n\\nTrain results:\")\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"precision is:\",metrics.precision_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"recall is:\",metrics.recall_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"f1 is:\",metrics.f1_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"---------------------\")\n",
    "print(\"Test results:\")\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"precision is:\",metrics.precision_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"recall is:\",metrics.recall_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"f1 is:\",metrics.f1_score(y_pred = ytest_pred, y_true = ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2890, 26)\n",
      "1445.0\n",
      "Xtrain shape: (2312, 12)\n",
      "ytrain shape: (2312,)\n",
      "Xtest shape: (578, 12)\n",
      "ytest shape: (578,)\n",
      "\n",
      "\n",
      "Train results:\n",
      "accuracy is: 0.6600346020761245\n",
      "precision is: 0.6632173095014111\n",
      "recall is: 0.6222418358340689\n",
      "f1 is: 0.6420765027322405\n",
      "---------------------\n",
      "Test results:\n",
      "accuracy is: 0.657439446366782\n",
      "precision is: 0.7050359712230215\n",
      "recall is: 0.6282051282051282\n",
      "f1 is: 0.6644067796610169\n"
     ]
    }
   ],
   "source": [
    "success_rate = 4200000\n",
    "\n",
    "pca_df.loc[(pca_df[\"status\"]==1)&(pca_df['total_raised']>=success_rate), 'suceeded'] = 1\n",
    "pca_df.loc[(pca_df[\"status\"]==0)|(pca_df['total_raised']<success_rate), 'suceeded'] = 0\n",
    "\n",
    "# print(pca_df.head())\n",
    "\n",
    "pca_df_suceeded = pca_df[pca_df['suceeded'] == 1]\n",
    "pca_df_failed = pca_df[pca_df['suceeded'] == 0]\n",
    "\n",
    "size = pca_df_suceeded.shape[0]\n",
    "pca_df_fialed_sampled = pca_df_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "equal_df = pd.concat([pca_df_suceeded, pca_df_fialed_sampled])\n",
    "\n",
    "print(equal_df.shape)\n",
    "print(equal_df['suceeded'].sum())\n",
    "\n",
    "pca_cols = [col for col in equal_df.columns if col not in cat_cols and col not in num_cols]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(equal_df[pca_cols], equal_df['suceeded'],test_size = 0.2)\n",
    "\n",
    "print(f\"Xtrain shape: {Xtrain.shape}\")\n",
    "print(f\"ytrain shape: {ytrain.shape}\")\n",
    "print(f\"Xtest shape: {Xtest.shape}\")\n",
    "print(f\"ytest shape: {ytest.shape}\")\n",
    "\n",
    "lr = LogisticRegression(max_iter=1500)\n",
    "lr.fit(Xtrain, ytrain)\n",
    "\n",
    "ytrain_pred = lr.predict(Xtrain)\n",
    "ytest_pred = lr.predict(Xtest)\n",
    "\n",
    "print(\"\\n\\nTrain results:\")\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"precision is:\",metrics.precision_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"recall is:\",metrics.recall_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"f1 is:\",metrics.f1_score(y_pred = ytrain_pred, y_true = ytrain))\n",
    "print(\"---------------------\")\n",
    "print(\"Test results:\")\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"precision is:\",metrics.precision_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"recall is:\",metrics.recall_score(y_pred = ytest_pred, y_true = ytest))\n",
    "print(\"f1 is:\",metrics.f1_score(y_pred = ytest_pred, y_true = ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.unsupervised_learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4054, 1902)\n",
      "2027.0\n"
     ]
    }
   ],
   "source": [
    "bin_df_suceeded = bin_df[bin_df['suceeded'] == 1]\n",
    "bin_df_failed = bin_df[bin_df['suceeded'] == 0]\n",
    "\n",
    "size = bin_df_suceeded.shape[0]\n",
    "bin_df_fialed_sampled = bin_df_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "equal_df = pd.concat([bin_df_suceeded, bin_df_fialed_sampled])\n",
    "\n",
    "print(equal_df.shape)\n",
    "print(equal_df['suceeded'].sum())\n",
    "\n",
    "# print(get_best_init_params_for_k_means(equal_df[bin_cols],8,['k-means++','random'],range(5,50,5),42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66970274 0.70824926 0.71654088 0.66332046 0.66203623 0.68207151\n",
      " 0.69582467 0.66819012 0.69711538 0.69798008 0.69903895 0.68300709\n",
      " 0.69748476 0.72990675 0.67123288]\n",
      "Best score after cross-validation: 0.7299067535220494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=15, test_size=0.2, random_state=42)\n",
    "clf = LogisticRegression(max_iter=150)\n",
    "\n",
    "scores = cross_val_score(clf, equal_df[bin_cols], equal_df['suceeded'], cv=cv, scoring='f1_macro')\n",
    "print(scores)\n",
    "print(f'Best score after cross-validation: {scores.max()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['precision_macro', 'recall_macro','f1_macro']\n",
    "scores = cross_validate(clf, equal_df[bin_cols], equal_df['suceeded'],cv = cv, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_macro: 0.7301166692084795\n",
      "test_recall_macro: 0.7299785663253155\n",
      "test_f1_macro: 0.7299067535220494\n",
      "---------------------\n",
      "train_precision_macro: 0.9012110170399743\n",
      "train_recall_macro: 0.9012798250691363\n",
      "train_f1_macro: 0.9012260996313126\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "for d in ['test','train']:\n",
    "    print(f\"{d}_precision_macro: {scores[f'{d}_precision_macro'].max()}\")\n",
    "    print(f\"{d}_recall_macro: {scores[f'{d}_recall_macro'].max()}\")\n",
    "    print(f\"{d}_f1_macro: {scores[f'{d}_f1_macro'].max()}\")\n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2992, 2864)\n",
      "1496.0\n",
      "test_precision_macro: 0.7113748320644873\n",
      "test_recall_macro: 0.710577974079279\n",
      "test_f1_macro: 0.710640139616056\n",
      "---------------------\n",
      "train_precision_macro: 0.9106606669544963\n",
      "train_recall_macro: 0.910609032524857\n",
      "train_f1_macro: 0.9105712381740049\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "df_outliers = pd.read_csv(\"data/dataframes/final_cleaned.csv\").iloc[:,1:]\n",
    "\n",
    "success_rate = 4000000\n",
    "\n",
    "df_outliers.loc[(df_outliers[\"status\"] == 1) & (df_outliers['total_raised'] >= success_rate), 'suceeded'] = 1\n",
    "df_outliers.loc[(df_outliers[\"status\"] == 0) | (df_outliers['total_raised'] < success_rate), 'suceeded'] = 0\n",
    "\n",
    "cat_cols = ['company_name', 'company_about','founded', 'business model','employees','product stage','status','fund_stage','suceeded']\n",
    "num_cols = ['total_raised','total_rounds', 'investors','ipo_price', 'geo_market_per']\n",
    "tag_cols = [col for col in df_outliers.columns if col.startswith('tag_')]\n",
    "targetmarket_cols = [col for col in df_outliers.columns if col.startswith('targetmarket_')]\n",
    "sector_list = [col for col in df_outliers.columns if col.startswith(\"sector_\")]\n",
    "target_ind_list = [col  for col in df_outliers.columns if col.startswith(\"industry_\")]\n",
    "technology_list = [col  for col in df_outliers.columns if col.startswith(\"technology_\")]\n",
    "\n",
    "\n",
    "pca_cols = [col for col in pca_df.columns if col not in cat_cols and col not in num_cols]\n",
    "bin_cols = tag_cols + targetmarket_cols + sector_list + target_ind_list + technology_list\n",
    "\n",
    "df_outliers_suceeded = df_outliers[df_outliers['suceeded'] == 1]\n",
    "df_outliers_failed = df_outliers[df_outliers['suceeded'] == 0]\n",
    "\n",
    "size = df_outliers_suceeded.shape[0]\n",
    "df_outliers_fialed_sampled = df_outliers_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "equal_df = pd.concat([df_outliers_suceeded, df_outliers_fialed_sampled])\n",
    "\n",
    "print(equal_df.shape)\n",
    "print(equal_df['suceeded'].sum())\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=15, test_size=0.2, random_state=42)\n",
    "clf = LogisticRegression(max_iter=150)\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro','f1_macro']\n",
    "scores = cross_validate(clf, equal_df[bin_cols], equal_df['suceeded'],cv = cv, scoring=scoring, return_train_score=True)\n",
    "\n",
    "for d in ['test','train']:\n",
    "    print(f\"{d}_precision_macro: {scores[f'{d}_precision_macro'].max()}\")\n",
    "    print(f\"{d}_recall_macro: {scores[f'{d}_recall_macro'].max()}\")\n",
    "    print(f\"{d}_f1_macro: {scores[f'{d}_f1_macro'].max()}\")\n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d8353be1dd092e57b3f2779bafc40fd4a1c87861698b48ff47a5f1df7325f59"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
