{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with importing the modules and loading the 3 dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from lib.unsupervised_learning import *\n",
    "\n",
    "from sklearn import svm, tree, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary dataframe shape: (10070, 1927)\n",
      "PCA dataframe shape: (10070, 1236)\n"
     ]
    }
   ],
   "source": [
    "bin_df = pd.read_csv('data/dataframes/df_after_cols_reduction.csv').iloc[:,1:]\n",
    "pca_df = pd.read_csv('data/dataframes/pca_df.csv').iloc[:,1:]\n",
    "df_no_outliers = pd.read_csv('data/dataframes/cleaned_no_outliers.csv').iloc[:,1:]\n",
    "# pca_2d_df = pd.read_csv('data/dataframes/pca_2d_df.csv').iloc[:,1:]\n",
    "# pca_3d_df = pd.read_csv('data/dataframes/pca_3d_df.csv').iloc[:,1:]\n",
    "\n",
    "print(f'Binary dataframe shape: {bin_df.shape}')\n",
    "print(f'PCA dataframe shape: {pca_df.shape}')\n",
    "print(f'No outliers dataframe shape: {df_no_outliers.shape}')\n",
    "# print(f'PCA 2D dataframe shape: {pca_2d_df.shape}')\n",
    "# print(f'PCA 3D dataframe shape: {pca_3d_df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['company_name', 'company_about','founded', 'business model','employees','product stage','status','funding stage','succeeded']\n",
    "num_cols = ['total_raised','total_rounds', 'investors','ipo_price', 'geo_market_per']\n",
    "tag_cols = [col for col in bin_df.columns if col.startswith('tag_')]\n",
    "targetmarket_cols = [col for col in bin_df.columns if col.startswith('targetmarket_')]\n",
    "sector_list = [col for col in bin_df.columns if col.startswith(\"sector_\")]\n",
    "target_ind_list = [col  for col in bin_df.columns if col.startswith(\"industry_\")]\n",
    "technology_list = [col  for col in bin_df.columns if col.startswith(\"technology_\")]\n",
    "\n",
    "\n",
    "bin_cols = tag_cols + targetmarket_cols + sector_list + target_ind_list + technology_list\n",
    "pca_cols = [col for col in pca_df.columns if col not in cat_cols]\n",
    "# pca_2d_cols = [col for col in pca_2d_df.columns if col not in cat_cols and col not in num_cols]\n",
    "# pca_3d_cols = [col for col in pca_3d_df.columns if col not in cat_cols and col not in num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cols : 9\n",
      "Numerical cols : 5\n",
      "Tag cols : 1599\n",
      "Targetmarket cols : 117\n",
      "Sector cols : 41\n",
      "Industry cols : 81\n",
      "Technology cols : 75\n",
      "---- Totals ----\n",
      "Total binary cols : 1913\n",
      "Total PCA cols : 1227\n"
     ]
    }
   ],
   "source": [
    "print(f\"Categorical cols : {len(cat_cols)}\")\n",
    "print(f\"Numerical cols : {len(num_cols)}\")\n",
    "print(f\"Tag cols : {len(tag_cols)}\")\n",
    "print(f\"Targetmarket cols : {len(targetmarket_cols)}\")\n",
    "print(f\"Sector cols : {len(sector_list)}\")\n",
    "print(f\"Industry cols : {len(target_ind_list)}\")\n",
    "print(f\"Technology cols : {len(technology_list)}\")\n",
    "print('---- Totals ----')\n",
    "print(f\"Total binary cols : {len(bin_cols)}\")\n",
    "print(f\"Total PCA cols : {len(pca_cols)}\")\n",
    "# print(f\"Toatl PCA 2D cols : {len(pca_2d_cols)}\")\n",
    "# print(f\"Total PCA 3D cols : {len(pca_3d_cols)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this noteook, we will train few machine learning models on our datasets to find the best model to predict the target variable.  \n",
    "The models we will use are:\n",
    "- [Logistic Regression](#lr)\n",
    "- [K-Nearest Neighbours](#knn)\n",
    "- [Support Vector Machine](#svm)\n",
    "- [Gaussian Naive Bayes](#gnb)\n",
    "- [Decision Tree](#dt)\n",
    "- [Random Forest](#rf)\n",
    "- [Multi-layer Perceptron](#mlp)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to prepare our data.  \n",
    "As we saw in the visualizations, we have almost 70% of successful companies in the dataset.  \n",
    "In order to avoid biased results, we need to train the models on an evenly distributed succeeded column in the dataset.  \n",
    "i.e the train data should contain 50% succesfull companies and 50% failed companies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binary data:\n",
      "Binary equal dataframe shape: (6122, 1927)\n",
      "Binary equal dataframe succeeded companies: 3061.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBinary data:\")\n",
    "bin_df_succeeded = bin_df[bin_df['succeeded'] == 1]\n",
    "bin_df_failed = bin_df[bin_df['succeeded'] == 0]\n",
    "\n",
    "size = min(bin_df_succeeded.shape[0], bin_df_failed.shape[0])\n",
    "\n",
    "bin_df_succeded_sampled = bin_df_succeeded.sample(n = size , random_state = 42)\n",
    "bin_df_failed_sampled = bin_df_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "bin_equal_df = pd.concat([bin_df_succeded_sampled, bin_df_failed_sampled])\n",
    "\n",
    "print(f'Binary equal dataframe shape: {bin_equal_df.shape}')\n",
    "print(f\"Binary equal dataframe succeeded companies: {bin_equal_df['succeeded'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same for both pca dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA data:\n",
      "PCA equal dataframe shape: (6122, 1236)\n",
      "PCA equal dataframe succeeded companies: 3061.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPCA data:\")\n",
    "\n",
    "pca_succeeded = pca_df[pca_df['succeeded'] == 1]\n",
    "pca_failed = pca_df[pca_df['succeeded'] == 0]\n",
    "\n",
    "size = min(pca_succeeded.shape[0], pca_failed.shape[0])\n",
    "\n",
    "pca_df_succeded_sampled = pca_succeeded.sample(n = size , random_state = 42)\n",
    "pca_df_failed_sampled = pca_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "pca_equal_df = pd.concat([pca_df_succeded_sampled, pca_df_failed_sampled])\n",
    "\n",
    "print(f'PCA equal dataframe shape: {pca_equal_df.shape}')\n",
    "print(f\"PCA equal dataframe succeeded companies: {pca_equal_df['succeeded'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n2D PCA data:\")\n",
    "\n",
    "# pca_2d_df_succeeded = pca_2d_df[pca_2d_df['succeeded'] == 1]\n",
    "# pca_2d_df_failed = pca_2d_df[pca_2d_df['succeeded'] == 0]\n",
    "\n",
    "# size = min(pca_2d_df_succeeded.shape[0], pca_2d_df_failed.shape[0])\n",
    "\n",
    "# pca_2d_df_succeded_sampled = pca_2d_df_succeeded.sample(n = size , random_state = 42)\n",
    "# pca_2d_df_failed_sampled = pca_2d_df_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "# pca_2d_equal_df = pd.concat([pca_2d_df_succeded_sampled, pca_2d_df_failed_sampled])\n",
    "\n",
    "# print(f'2D PCA equal dataframe shape: {pca_2d_equal_df.shape}')\n",
    "# print(f\"2D PCA equal dataframe succeeded companies: {pca_2d_equal_df['succeeded'].sum()}\")\n",
    "\n",
    "# print(\"\\n3D PCA data:\")\n",
    "\n",
    "# pca_3d_df_succeeded = pca_3d_df[pca_3d_df['succeeded'] == 1]\n",
    "# pca_3d_df_failed = pca_3d_df[pca_3d_df['succeeded'] == 0]\n",
    "\n",
    "# size = min(pca_3d_df_succeeded.shape[0], pca_3d_df_failed.shape[0])\n",
    "\n",
    "# pca_3d_df_succeded_sampled = pca_3d_df_succeeded.sample(n = size , random_state = 42)\n",
    "# pca_3d_df_failed_sampled = pca_3d_df_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "# pca_3d_equal_df = pd.concat([pca_3d_df_succeded_sampled, pca_3d_df_failed_sampled])\n",
    "\n",
    "# print(f'3D PCA equal dataframe shape: {pca_3d_equal_df.shape}')\n",
    "# print(f\"3D PCA equal dataframe succeeded companies: {pca_3d_equal_df['succeeded'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_XTrain shape: (4897, 1913)\n",
      "bin_yTrain shape: (4897,)\n",
      "bin_XTest shape: (1225, 1913)\n",
      "bin_yTest shape: (1225,)\n",
      "pca_XTrain shape: (4897, 1227)\n",
      "pca_yTrain shape: (4897,)\n",
      "pca_XTest shape: (1225, 1227)\n",
      "pca_yTest shape: (1225,)\n"
     ]
    }
   ],
   "source": [
    "bin_XTrain, bin_XTest, bin_yTrain, bin_yTest = train_test_split(bin_equal_df[bin_cols], bin_equal_df['succeeded'], test_size=0.2, random_state=42, stratify=bin_equal_df['succeeded'])\n",
    "pca_XTrain, pca_XTest, pca_yTrain, pca_yTest = train_test_split(pca_equal_df[pca_cols], pca_equal_df['succeeded'], test_size=0.2, random_state=42, stratify=pca_equal_df['succeeded'])\n",
    "# pca_2d_XTrain, pca_2d_XTest, pca_2d_yTrain, pca_2d_yTest = train_test_split(pca_2d_equal_df[pca_2d_cols], pca_2d_equal_df['succeeded'], test_size=0.2, random_state=42, stratify=pca_2d_equal_df['succeeded'])\n",
    "# pca_3d_XTrain, pca_3d_XTest, pca_3d_yTrain, pca_3d_yTest = train_test_split(pca_3d_equal_df[pca_3d_cols], pca_3d_equal_df['succeeded'], test_size=0.2, random_state=42, stratify=pca_3d_equal_df['succeeded'])\n",
    "\n",
    "\n",
    "print(f\"bin_XTrain shape: {bin_XTrain.shape}\")\n",
    "print(f\"bin_yTrain shape: {bin_yTrain.shape}\")\n",
    "\n",
    "print(f\"bin_XTest shape: {bin_XTest.shape}\")\n",
    "print(f\"bin_yTest shape: {bin_yTest.shape}\")\n",
    "\n",
    "print(f\"pca_XTrain shape: {pca_XTrain.shape}\")\n",
    "print(f\"pca_yTrain shape: {pca_yTrain.shape}\")\n",
    "\n",
    "print(f\"pca_XTest shape: {pca_XTest.shape}\")\n",
    "print(f\"pca_yTest shape: {pca_yTest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create functions to train different models and return the scores :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "we will use **ShuffleSplit** and **cross validation** to get the best results of the model. \n",
    "<a id='lr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(XTrain, yTrain, XTest, yTest):\n",
    "    cv = ShuffleSplit(n_splits=15, test_size=0.2, random_state=42)\n",
    "    clf = LogisticRegression(max_iter=150)\n",
    "\n",
    "    lr = GridSearchCV(clf, param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}, cv=cv, scoring=scoring)\n",
    "    lr.fit(XTrain, yTrain)\n",
    "    y_pred = lr.predict(XTest)\n",
    "\n",
    "    f1 = metrics.f1_score(yTest, y_pred)\n",
    "    accuracy = metrics.accuracy_score(yTest, y_pred)\n",
    "    \n",
    "    return  {'test_f1_macro': f1, 'test_accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbours\n",
    "we will use **GridSearch** to find best hyper-parameters for KNN algorithm. \n",
    "<a id='knn'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn(XTrain, yTrain, XTest, yTest):\n",
    "\n",
    "    parameters = {'n_neighbors':range(2,50,2), 'weights':['uniform', 'distance'], }\n",
    "    knn = KNeighborsClassifier()\n",
    "    clf = GridSearchCV(knn, parameters,scoring=scoring)\n",
    "    clf.fit(XTrain, yTrain)\n",
    "    y_pred = clf.predict(XTest)\n",
    "\n",
    "    f1 = metrics.f1_score(yTest, y_pred)\n",
    "    accuracy = metrics.accuracy_score(yTest, y_pred)\n",
    "    \n",
    "    return  {'test_f1_macro': f1, 'test_accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "we will use **GridSearch** to find best hyper-parameters for SVC algorithm. \n",
    "<a id='svc'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(XTrain, yTrain, XTest, yTest):\n",
    "    parameters = {'C':[0.1,1,10], 'kernel':['linear', 'rbf']}\n",
    "    s = svm.SVC()\n",
    "    clf = GridSearchCV(s, parameters,scoring=scoring)\n",
    "    clf.fit(XTrain, yTrain)\n",
    "    y_pred = clf.predict(XTest)\n",
    "    \n",
    "    f1 = metrics.f1_score(yTest, y_pred)\n",
    "    accuracy = metrics.accuracy_score(yTest, y_pred)\n",
    "    \n",
    "    return  {'test_f1_macro': f1, 'test_accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "<a id='gnb'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnb(XTrain, yTrain, XTest, yTest):\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(XTrain, yTrain)\n",
    "    y_pred = gnb.predict(XTest)\n",
    "    f1 = metrics.f1_score(yTest, y_pred)\n",
    "    accuracy = metrics.accuracy_score(yTest, y_pred)\n",
    "\n",
    "    return  {'test_f1_macro': f1, 'test_accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "<a id='dt'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dt(XTrain, yTrain, XTest, yTest):\n",
    "    parameters = {'max_depth':range(2,20,2), 'min_samples_split':range(2,20,2), 'min_samples_leaf':range(2,20,2)}\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    dt = GridSearchCV(clf, parameters ,scoring=scoring)\n",
    "    dt.fit(XTrain, yTrain)\n",
    "    y_pred = dt.predict(XTest)\n",
    "    \n",
    "    f1 = metrics.f1_score(yTest, y_pred)\n",
    "    accuracy = metrics.accuracy_score(yTest, y_pred)\n",
    "    \n",
    "    return  {'test_f1_macro': f1, 'test_accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "<a id='rf'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(XTrain, yTrain, XTest, yTest):\n",
    "    parameters = {'n_estimators':range(2,50,2), 'max_depth':range(2,20,2), 'min_samples_split':range(2,20,2), 'min_samples_leaf':range(2,20,2)}\n",
    "    clf = RandomForestClassifier()\n",
    "    rf = GridSearchCV(clf, parameters ,scoring=scoring)\n",
    "    rf.fit(XTrain, yTrain)\n",
    "    y_pred = rf.predict(XTest)\n",
    "    \n",
    "    f1 = metrics.f1_score(yTest, y_pred)\n",
    "    accuracy = metrics.accuracy_score(yTest, y_pred)\n",
    "    \n",
    "    return  {'test_f1_macro': f1, 'test_accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network - Multi-layer Perceptron\n",
    "<a id='mlp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(XTrain, yTrain, XTest, yTest):\n",
    "    parameters = {'hidden_layer_sizes':[(100,), (50,), (25,), (10,)], 'activation':['identity', 'logistic', 'tanh', 'relu'], 'solver':['adam']}\n",
    "    clf = MLPClassifier(max_iter=500, random_state=42)\n",
    "    mlp = GridSearchCV(clf, parameters ,scoring=scoring)\n",
    "    mlp.fit(XTrain, yTrain)\n",
    "    y_pred = mlp.predict(XTest)\n",
    "\n",
    "    f1 = metrics.f1_score(yTest, y_pred)\n",
    "    accuracy = metrics.accuracy_score(yTest, y_pred)\n",
    "    \n",
    "    return  {'test_f1_macro': f1, 'test_accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will run every algorithm on each dataset and compare the results  \n",
    "We will use **tqdm** and **timeit** libraries to measure the time it takes to train all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e237bdf6dd84934842ea0719d7e3619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:19.672245\n",
      "pca_df Model LogisticRegression: \n",
      "{'test_f1_macro': 0.25396825396825395, 'test_accuracy': 0.5395918367346939}\n",
      "\n",
      "Elapsed time: 0:01:06.041475\n",
      "pca_df Model KNN: \n",
      "{'test_f1_macro': 0.5365384615384615, 'test_accuracy': 0.6065306122448979}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# runtime ~ 55 minutes\n",
    "import tqdm.notebook as tqdm\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "\n",
    "dfs_scores = {}\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "dfs ={'pca_df':(pca_equal_df, pca_cols), 'bin_df': (bin_equal_df,bin_cols)}#'pca_2d_df': (pca_2d_df,pca_2d_cols), 'pca_3d_df': (pca_3d_df,pca_3d_cols)}\n",
    "with tqdm.tqdm(total=len(dfs)*7) as pbar:\n",
    "    for key, df in dfs.items():\n",
    "        scores = {} \n",
    "        \n",
    "        XTrain, XTest, yTrain, yTest = train_test_split(df[0][df[1]], df[0]['succeeded'], test_size=0.2, random_state=42, stratify=df[0]['succeeded'])\n",
    "\n",
    "        scores['LogisticRegression'] = train_logistic_regression(XTrain, yTrain, XTest, yTest)\n",
    "        print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model LogisticRegression: \\n{scores[\"LogisticRegression\"]}\\n')\n",
    "        pbar.update(1)\n",
    "\n",
    "        scores['KNN'] = train_knn(XTrain, yTrain, XTest, yTest)\n",
    "        print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model KNN: \\n{scores[\"KNN\"]}\\n')\n",
    "        pbar.update(1)\n",
    "\n",
    "        scores['SVM'] = train_svm(XTrain, yTrain, XTest, yTest)\n",
    "        print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model SVM: \\n{scores[\"SVM\"]}\\n')\n",
    "        pbar.update(1)\n",
    "\n",
    "        scores['GNB'] = train_gnb(XTrain, yTrain, XTest, yTest)\n",
    "        print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model GNB: \\n{scores[\"GNB\"]}\\n')\n",
    "        pbar.update(1)\n",
    "\n",
    "        scores['DT'] = train_dt(XTrain, yTrain, XTest, yTest)\n",
    "        print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model DT: \\n{scores[\"DT\"]}\\n')\n",
    "        pbar.update(1)\n",
    "\n",
    "        scores['RF'] = train_rf(XTrain, yTrain, XTest, yTest)\n",
    "        print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model RF: \\n{scores[\"RF\"]}\\n')\n",
    "        pbar.update(1)\n",
    "\n",
    "        scores['MLP'] = train_mlp(XTrain, yTrain, XTest, yTest)\n",
    "        print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model MLP: \\n{scores[\"MLP\"]}\\n')\n",
    "        pbar.update(1)\n",
    "\n",
    "        dfs_scores[key] = scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = pd.read_csv('data/dataframes/cleaned_no_outliers.csv').iloc[:,1:]\n",
    "\n",
    "tag_cols = [col for col in df_no_outliers.columns if col.startswith('tag_')]\n",
    "targetmarket_cols = [col for col in df_no_outliers.columns if col.startswith('targetmarket_')]\n",
    "sector_list = [col for col in df_no_outliers.columns if col.startswith(\"sector_\")]\n",
    "target_ind_list = [col  for col in df_no_outliers.columns if col.startswith(\"industry_\")]\n",
    "technology_list = [col  for col in df_no_outliers.columns if col.startswith(\"technology_\")]\n",
    "\n",
    "bin_cols = tag_cols + targetmarket_cols + sector_list + target_ind_list + technology_list\n",
    "\n",
    "bin_df_succeeded = df_no_outliers[df_no_outliers['succeeded'] == 1]\n",
    "bin_df_failed = df_no_outliers[df_no_outliers['succeeded'] == 0]\n",
    "\n",
    "size = min(bin_df_succeeded.shape[0], bin_df_failed.shape[0])\n",
    "\n",
    "bin_df_succeded_sampled = bin_df_succeeded.sample(n = size , random_state = 42)\n",
    "bin_df_failed_sampled = bin_df_failed.sample(n = size , random_state = 42)\n",
    "\n",
    "bin_equal_df = pd.concat([bin_df_succeded_sampled, bin_df_failed_sampled])\n",
    "\n",
    "print(f'Binary equal dataframe shape: {bin_equal_df.shape}')\n",
    "print(f\"Binary equal dataframe succeeded companies: {bin_equal_df['succeeded'].sum()}\")\n",
    "\n",
    "scores = {} \n",
    "        \n",
    "XTrain, XTest, yTrain, yTest = train_test_split(bin_equal_df[bin_cols], bin_equal_df['succeeded'], test_size=0.2, random_state=42, stratify=bin_equal_df['succeeded'])\n",
    "\n",
    "scores['LogisticRegression'] = train_logistic_regression(XTrain, yTrain, XTest, yTest)\n",
    "print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model LogisticRegression: \\n{scores[\"LogisticRegression\"]}\\n')\n",
    "\n",
    "scores['KNN'] = train_knn(XTrain, yTrain, XTest, yTest)\n",
    "print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model KNN: \\n{scores[\"KNN\"]}\\n')\n",
    "\n",
    "scores['SVM'] = train_svm(XTrain, yTrain, XTest, yTest)\n",
    "print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model SVM: \\n{scores[\"SVM\"]}\\n')\n",
    "\n",
    "scores['GNB'] = train_gnb(XTrain, yTrain, XTest, yTest)\n",
    "print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model GNB: \\n{scores[\"GNB\"]}\\n')\n",
    "\n",
    "scores['DT'] = train_dt(XTrain, yTrain, XTest, yTest)\n",
    "print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model DT: \\n{scores[\"DT\"]}\\n')\n",
    "\n",
    "scores['RF'] = train_rf(XTrain, yTrain, XTest, yTest)\n",
    "print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model RF: \\n{scores[\"RF\"]}\\n')\n",
    "\n",
    "scores['MLP'] = train_mlp(XTrain, yTrain, XTest, yTest)\n",
    "print(f'Elapsed time: {timedelta(seconds = timeit.default_timer() - t0)}\\n{key} Model MLP: \\n{scores[\"MLP\"]}\\n')\n",
    "\n",
    "dfs_scores['cleaned_no_outliers'] = scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the results for all the datasets, we will compare the results and find the best model.  \n",
    "First we will devide the results into 4 groups (F1 score, Accuracy, Precision, Recall) and then we will plot the data using plotly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {'LogisticRegression': [], 'KNN': [], 'SVM': [], 'GNB': [], 'DT': [], 'RF': [], 'MLP': []}\n",
    "\n",
    "for key, scores in dfs_scores.items():\n",
    "    for model, score in scores.items():\n",
    "        model_scores[model].append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_values = {}\n",
    "accuracy_values = {}\n",
    "precision_values = {}\n",
    "recall_values = {}\n",
    "for model, scores in model_scores.items():\n",
    "    f1_values[model] = [score['test_f1_macro'] for score in scores]\n",
    "    accuracy_values[model] = [score['test_accuracy'] for score in scores]\n",
    "    precision_values[model] = [score['test_precision_macro'] for score in scores]\n",
    "    recall_values[model] = [score['test_recall_macro'] for score in scores]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the data in 2 ways:\n",
    "- For each scoring function, we will plot the algorithms's scores in a bar chart.\n",
    "- For each Algorithm, we will plot the scores in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "x = ['Binary Data', 'PCA 2D Data', 'PCA 3D Data']\n",
    "titles = ['F1 Score', 'Accuracy Score', 'Precision Score', 'Recall Score']\n",
    "for i,score in enumerate([f1_values,accuracy_values]):\n",
    "    fig = go.Figure(data=[\n",
    "    go.Bar(name='Logistic Regression', x=x, y=list(score.values())[0]),\n",
    "    go.Bar(name='KNN', x=x, y=list(score.values())[1]),\n",
    "    go.Bar(name='SVM', x=x, y=list(score.values())[2]),\n",
    "    go.Bar(name='GNB', x=x, y=list(score.values())[3]),\n",
    "    go.Bar(name='DT', x=x, y=list(score.values())[4]),\n",
    "    go.Bar(name='RF', x=x, y=list(score.values())[5]),\n",
    "    go.Bar(name='MLP', x=x, y=list(score.values())[6])\n",
    "    ], layout=go.Layout(title=titles[i],title_x = 0.5,  barmode='group', xaxis=dict(title='DataFrame'), yaxis=dict(title=titles[i]), width=1200, height=500))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "algs = ['LogisticRegression','KNN','SVM','GNB','DT','RF','MLP']\n",
    "\n",
    "for i, alg in enumerate(algs):\n",
    "    fig = go.Figure(data=[\n",
    "    go.Bar(name='F1 Score', x=x, y=list(f1_values[alg])),\n",
    "    go.Bar(name='Accuracy_value', x=x, y=list(accuracy_values[alg])),\n",
    "    go.Bar(name='Precision_value', x=x, y=list(precision_values[alg])),\n",
    "    go.Bar(name='Recall_value', x=x, y=list(recall_values[alg]))\n",
    "    ], layout=go.Layout(title=algs[i],title_x = 0.5,  barmode='group', xaxis=dict(title='DataFrame'), yaxis=dict(title=algs[i]), width=1200, height=500))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From first glance:  \n",
    "We see that Logistic Regression didn't work so well in the Binary and PCA datasets.  \n",
    "GNB failed in the Binary dataset, but redeemed itself in the PCA dataset.  \n",
    "The rest of the model did well in both datasets - and returned high F1 and Accuracy scores on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to find the model with the best scores.  \n",
    "We will find the model with the highest F1 score and the model with highest Accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_model = ''\n",
    "best_accuracy_model = ''\n",
    "\n",
    "best_f1 = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for model, scores in model_scores.items():\n",
    "    f1 = [score['test_f1_macro'] for score in scores]\n",
    "    accuracy = [score['test_accuracy'] for score in scores]\n",
    "    max_f1 = max(f1)\n",
    "    max_accuracy = max(accuracy)\n",
    "    if max_f1 > best_f1:\n",
    "        best_f1 = max_f1\n",
    "        best_f1_model = model\n",
    "    if max_accuracy > best_accuracy:\n",
    "        best_accuracy = max_accuracy\n",
    "        best_accuracy_model = model\n",
    "\n",
    "print(\"-----Find the best model-----\")\n",
    "if best_f1_model == best_accuracy_model:\n",
    "    print(f'Best model is {best_f1_model}!')\n",
    "    print(f'It has f1 score of {best_f1} and accuracy score of {best_accuracy}')\n",
    "else:\n",
    "    print(f'Best model according to f1 score is {best_f1_model} with f1 score {best_f1}')\n",
    "    print(f'Best model according to accuracy score is {best_accuracy_model} with accuracy score {best_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we have trained several machine learning models on our datasets and compared the results.  \n",
    "We have found that the model best predicting the target variable (success or failure) is:  \n",
    "### **Random Forest** with **F1 score of 0.82 and Accuracy of 0.72**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was a great opportunity to learn about machine learning and to learn how to use the different algorithms.  \n",
    "The long journey we took tought us a lot about data in general, and specifically aout different methods to deal with it.  \n",
    "\n",
    "We were very pleased with the results of the project, and maybe this can have an actual real world implications.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d8353be1dd092e57b3f2779bafc40fd4a1c87861698b48ff47a5f1df7325f59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
